{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ilanazs/.local/lib/python3.6/site-packages/ants/viz/render_surface_function.py:16: UserWarning:\n",
      "\n",
      "Cant import Plotly. Install it `pip install chart_studio` if you want to use ants.render_surface_function\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import brainsss\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.signal import butter, filtfilt, freqz\n",
    "from scipy import signal\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import cm\n",
    "import random\n",
    "from scipy.stats import sem, zscore\n",
    "import time\n",
    "import h5py\n",
    "import ants\n",
    "import nibabel as nib\n",
    "import matplotlib\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import pickle\n",
    "from skimage import io\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "later_dir = '/oak/stanford/groups/trc/data/Ilana/2P/data/later/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fly_208_dff_dic.pkl\n",
      "fly_208_dic.pkl\n",
      "fly_209_dic.pkl\n",
      "fly_210_dic.pkl\n",
      "fly_217_dic.pkl\n",
      "fly_218_dic.pkl\n",
      "fly_226_dic.pkl\n",
      "fly_227_dic.pkl\n",
      "fly_228_dic.pkl\n",
      "fly_233_dic.pkl\n",
      "fly_234_dic.pkl\n",
      "fly_239_dic.pkl\n",
      "fly_240_dic.pkl\n",
      "fly_241_dic.pkl\n",
      "fly_242_dic.pkl\n",
      "fly_249_dic.pkl\n",
      "fly_250_dic.pkl\n",
      "total_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "for x in sorted(os.listdir(later_dir)):\n",
    "    if 'dic' in x:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading total dict\n",
      "CPU times: user 4.9 s, sys: 23 s, total: 27.9 s\n",
      "Wall time: 27.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_path = os.path.join(later_dir, 'total_dict.pkl')\n",
    "if os.path.exists(total_path)==False:\n",
    "    print(\"Making total dict\")\n",
    "    total_data_dict = {}\n",
    "\n",
    "    for x in sorted(os.listdir(later_dir)):\n",
    "        if 'dic' in x:\n",
    "            fly_name= x[4:7]\n",
    "            temp_path = os.path.join(later_dir, x)\n",
    "            with open(temp_path, 'rb') as file:\n",
    "                dic = pickle.load(file)\n",
    "                total_data_dict[fly_name] = dic\n",
    "    with open(total_path, 'wb') as file:\n",
    "        pickle.dump(total_data_dict, file)\n",
    "else:\n",
    "    print(\"Loading total dict\")\n",
    "    with open(total_path, 'rb') as file:\n",
    "        total_data_dict = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['208', '209', '210', '217', '218', '226', '227', '228', '233', '234', '239', '240', '241', '242', '249', '250'])\n",
      "dict_keys(['signals', 'timestamps', 'event_times', 'bins', 'behavior', 'cluster_labels'])\n"
     ]
    }
   ],
   "source": [
    "print(total_data_dict.keys())\n",
    "print(total_data_dict['208'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3384, 49)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(total_data_dict['208']['timestamps']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total_behave_struct = {}\n",
    "shape = 196 #this is hard coded to be the smallest but i gotta figure this shit out better\n",
    "for x in sorted(os.listdir(later_dir)):\n",
    "    if 'dic' in x and 'total' not in x:\n",
    "        fly_name= x[4:7]\n",
    "        temp_path = os.path.join(later_dir, x)\n",
    "        with open(temp_path, 'rb') as file:\n",
    "            struct = pickle.load(file)\n",
    "            total_behave_struct[fly_name] = struct['behavior'][:shape,:]\n",
    "            temp = struct['behavior']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(total_behave_struct.keys())\n",
    "print(np.shape(total_behave_struct['208']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.axvline(200, linewidth=2, linestyle='--')\n",
    "ax.axvline(300, linewidth=2, linestyle='--')\n",
    "# for key in total_behave_struct:\n",
    "plt.plot(total_behave_struct['217'].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_boxcar_filter(arr):\n",
    "    boxcar = np.ones(3) / 3  # Define the boxcar filter\n",
    "    print(boxcar)\n",
    "    filtered_arr = np.convolve(arr, boxcar, mode='same')  # Apply the filter\n",
    "    return filtered_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd = apply_boxcar_filter(total_behave_struct['240'][100,:])\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(total_behave_struct['234'][100:105,:].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    return butter(order, cutoff, fs=fs, btype='low', analog=False)\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data, method=\"gust\")\n",
    "    return y\n",
    "def apply_butter_lowpass(behavior_traces, fr):\n",
    "    # Filter requirements.\n",
    "    order = 4\n",
    "    fs = fr      # sample rate, Hz\n",
    "    cutoff = 3  # desired cutoff frequency of the filter, Hz\n",
    "    \n",
    "    lpf_behavior = []\n",
    "    \n",
    "    for i in range(np.shape(behavior_traces)[0]):\n",
    "        temp = butter_lowpass_filter(behavior_traces[i,:], cutoff, fs, order)\n",
    "        lpf_behavior.append(temp)\n",
    "    \n",
    "    return np.asarray(lpf_behavior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr=100\n",
    "lpf=apply_butter_lowpass(total_behave_struct['208'], fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(lpf))\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(total_behave_struct['208'][0,:].T);\n",
    "ax.plot(lpf[0,:].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.axvline(200, linewidth=2, linestyle='--')\n",
    "ax.axvline(300, linewidth=2, linestyle='--')\n",
    "# for key in total_behave_struct:\n",
    "plt.plot(lpf.T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_trials = {}\n",
    "count=0\n",
    "for trial in total_behave_struct['208']:\n",
    "    num_neg = np.count_nonzero(trial[:200]<-1)\n",
    "    neg_trials[count]=num_neg\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0\n",
    "for key in neg_trials:\n",
    "    if neg_trials[key]>=10:\n",
    "        num+=1\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fly = '209'\n",
    "# thresh_val = 0.01\n",
    "\n",
    "# weird_idx = np.where(np.count_nonzero(total_behave_struct[fly][:, :200]<-1, axis=1)>5)\n",
    "# print(np.shape(weird_idx))\n",
    "# greater = np.where(np.mean(total_behave_struct[fly][:, :200], axis=1)>thresh_val)\n",
    "# # print(greater)\n",
    "# print(np.shape(greater))\n",
    "# less = np.where(np.mean(total_behave_struct[fly][:, :200], axis=1)<=thresh_val)\n",
    "# print(np.shape(less))\n",
    "# greater_s = np.asarray(list(set(greater[0])-set(weird_idx[0])))\n",
    "# less_s = np.asarray(list(set(less[0])-set(weird_idx[0])))\n",
    "# print(np.shape(greater_s))\n",
    "# print(np.shape(less_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get bin of trials where vel is less than -1 or something (neg trials, 0 trials, moving trials, weird trials)\n",
    "thresh_val = 0.01\n",
    "divided_behave = {\"greater\":{},\"less\":{}, \"weird\":{}}\n",
    "gtotal = 0\n",
    "ltotal = 0\n",
    "wtotal = 0\n",
    "for fly in total_behave_struct:\n",
    "    weird = np.where(np.count_nonzero(total_behave_struct[fly][:, :200]<-1, axis=1)>10)\n",
    "    divided_behave['weird'][fly] = total_behave_struct[fly][weird]\n",
    "    wcount = np.shape(weird)[1]\n",
    "    wtotal+=wcount\n",
    "    print('Fly {} weird: {}'.format(fly, wcount))\n",
    "    greater = np.where(np.mean(total_behave_struct[fly][:, :200], axis=1)>thresh_val)\n",
    "    greater = np.asarray(list(set(greater[0])-set(weird[0])))\n",
    "    if greater != []:\n",
    "        divided_behave['greater'][fly] = total_behave_struct[fly][greater]\n",
    "        gcount = np.shape(greater)[0]\n",
    "        gtotal+=gcount\n",
    "    else:\n",
    "        gcount = 0\n",
    "    print('Fly {} greater: {}'.format(fly, gcount))\n",
    "    less = np.where(np.mean(total_behave_struct[fly][:, :200], axis=1)<=thresh_val)\n",
    "    less = np.asarray(list(set(less[0])-set(weird[0])))\n",
    "#     print(less)\n",
    "    if less != []:\n",
    "        divided_behave['less'][fly] = total_behave_struct[fly][less]\n",
    "        lcount = np.shape(less)[0]\n",
    "        ltotal+=lcount\n",
    "    else:\n",
    "        lcount=0\n",
    "    print('Fly {} less: {}'.format(fly, lcount))\n",
    "print('Greater total: {}'.format(gtotal))\n",
    "print('Less total: {}'.format(ltotal))\n",
    "print('Weird total: {}'.format(wtotal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtot = 0\n",
    "for fly in total_behave_struct:\n",
    "    g = np.where(np.mean(total_behave_struct[fly][:, 190:200], axis=1)>=3)\n",
    "    gc = np.shape(g)[1]\n",
    "#     gc = np.shape(g)[0]\n",
    "    gtot+=gc\n",
    "    print('Fly {} greater or equal to thresh: {}'.format(fly, gc))\n",
    "print('Greater total: {}'.format(gtot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_flies(struct):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "#     ax.axvline(200, linewidth=2, linestyle='--')\n",
    "#     ax.axvline(300, linewidth=2, linestyle='--')\n",
    "    for key in struct:\n",
    "        f=np.mean(struct[key], axis=0)\n",
    "    #     print(np.shape(f))\n",
    "        ax.plot(f.T, label=key)\n",
    "        ax.legend()\n",
    "    # # ax.plot(np.mean(total_behave, axis=1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flies(total_behave_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flies(divided_behave['greater'])\n",
    "plot_flies(divided_behave['less'])\n",
    "plot_flies(divided_behave['weird'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm_behave(struct, norm=None):\n",
    "    norm_struct = {}\n",
    "    for fly in struct:\n",
    "        temp = struct[fly]\n",
    "        pre_v = temp[:,100:200]\n",
    "        mean_pre = np.mean(pre_v)\n",
    "        std_pre = np.std(pre_v)\n",
    "        if norm=='zscore':\n",
    "            temp_norm = (temp-mean_pre)/std_pre\n",
    "        elif norm=='meanscore':\n",
    "            temp_norm = (temp-mean_pre)/mean_pre\n",
    "        elif norm=='clark':\n",
    "            temp_norm = temp/mean_pre\n",
    "        elif norm=='scipy':\n",
    "            temp_norm = zscore(temp, axis=1)\n",
    "        else:\n",
    "            print(\"No established norm type. Try again. Options: zscore, meanscore, clark, scipy.\")\n",
    "        try:\n",
    "            norm_struct[fly]=temp_norm\n",
    "        except:\n",
    "            break\n",
    "    return norm_struct\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flies(norm_behave(divided_behave['greater'], norm='clark'))\n",
    "# plot_flies(norm_behave(divided_behave['greater'], norm='meanscore'))\n",
    "plot_flies(norm_behave(divided_behave['greater'], norm='zscore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zs=norm_behave(total_behave_struct, norm='zscore')\n",
    "meanscore=norm_behave(total_behave_struct, norm='meanscore')\n",
    "social=norm_behave(total_behave_struct, norm='clark')\n",
    "sp=norm_behave(total_behave_struct, norm='scipy')\n",
    "gsocial=norm_behave(divided_behave['greater'], norm='clark')\n",
    "er_test=norm_behave(total_behave_struct, norm=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fly = \"240\"\n",
    "\n",
    "a=np.mean(total_data_dict[fly]['behavior'], axis=0)\n",
    "# print(np.shape(a))\n",
    "b=np.mean(zs[fly], axis=0)\n",
    "# print(np.shape(b))\n",
    "c=np.mean(meanscore[fly], axis=0)\n",
    "d=np.mean(social[fly], axis=0)\n",
    "e=np.mean(sp[fly], axis=0)\n",
    "f=np.mean(divided_behave['greater'][fly], axis=0)\n",
    "g=np.mean(gsocial[fly], axis=0)\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "# fig.subplots(figsize=(10,10))\n",
    "ax.plot(a.T, label='No normalization')\n",
    "ax.plot(b.T, label='z-score')\n",
    "ax.plot(c.T, label='mean-score')\n",
    "ax.plot(d.T, label='social behavior')\n",
    "ax.plot(e.T, label='scipy')\n",
    "ax.plot(f.T, label='greater')\n",
    "ax.plot(g.T, label='greater social')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_flies(total_behave_struct)\n",
    "plot_flies(social)\n",
    "plot_flies(gsocial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trace(behavior_trace, pre_window, post_window, stim_time, ax=None, fig=None):\n",
    "    mean_trace = np.mean(behavior_trace, axis=0)\n",
    "    sem_trace = scipy.stats.sem(behavior_trace, axis=0)\n",
    "    color='b'\n",
    "    \n",
    "    if ax==None:\n",
    "        fig, ax = plt.subplots(figsize=(10,10))\n",
    "        color='k'\n",
    "#     plt.figure(figsize=(10,10))\n",
    "    ax.plot(mean_trace,color=color,linewidth=3)\n",
    "    ax.fill_between(np.arange(np.shape(mean_trace)[0]),mean_trace-sem_trace, mean_trace+sem_trace, color='k',alpha=0.3)\n",
    "    ax.axvline(pre_window,color='k',linestyle='--',lw=2)\n",
    "    ax.axvline(pre_window+stim_time,color='k',linestyle='--',lw=2)\n",
    "    ax.set_ylim(-0.5, 3.5);\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overall_behavior_array(struct):\n",
    "    temp = []\n",
    "    for key in struct:\n",
    "        try:\n",
    "            lst = struct[key]['traces']\n",
    "        except IndexError:\n",
    "            lst = struct[key]\n",
    "#         print(np.shape(lst))\n",
    "        temp.append(lst)\n",
    "    temp = np.vstack(temp)\n",
    "#     print(np.shape(temp))\n",
    "#     temp = np.mean(temp, axis=0)\n",
    "    return np.asarray(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = overall_behavior_array(total_behave_struct)\n",
    "total_norm = overall_behavior_array(social)\n",
    "\n",
    "total1 = overall_behavior_array(divided_behave['greater'])\n",
    "total_norm1 = overall_behavior_array(gsocial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_window = 200\n",
    "post_window = 300\n",
    "stim_time = 100\n",
    "fig, ax = plot_trace(total, pre_window, post_window, stim_time)\n",
    "# plot_trace(total_norm, pre_window, post_window, stim_time, ax=ax)\n",
    "\n",
    "fig1, ax1 = plot_trace(total1, pre_window, post_window, stim_time)\n",
    "plot_trace(total_norm1, pre_window, post_window, stim_time, ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value_struct ={\n",
    "    # the data is in 10ms, use this value to get the index if i use seconds\n",
    "    \"tp_width_sec\": 0.01,\n",
    "    \"trial_time_sec\": 1,\n",
    "    \"before_stim_sec\": pre_window/100,\n",
    "    # time in seconds between end of pre trial window and the stimulus time. \n",
    "    # when set to zero, the pre trial window ends when the stimulus is presented\n",
    "    \"pre_trial_window_sec\": 0.01,\n",
    "    # size of window that pre trial speed is averaged over, seconds, window ends at stimulus time\n",
    "    # when 0 is a single val and not a window\n",
    "    \"pre_trial_size_sec\": 0.5,\n",
    "    # size of post trial speed average window, sec\n",
    "    \"post_trial_window_sec\": 0.5,\n",
    "    # post trial window at stim tiem + post trail delay\n",
    "    # when 0 is a single val and not a window\n",
    "    \"post_trial_size_sec\": 0.5,\n",
    "    \"thresh\": 0.05,\n",
    "    \"fr\": 100.00033694743087 \n",
    "    #frame rate\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalized\n",
    "inc_n_dict, dec_n_dict, flat_n_dict = brainsss.separate_traces(social, value_struct)\n",
    "#only moving trials\n",
    "inc_dict, dec_dict, flat_dict = brainsss.separate_traces(gsocial, value_struct)\n",
    "#total\n",
    "inct, dect, flatt = brainsss.separate_traces(total_behave_struct, value_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotable_arrays(inc, dec, flat):\n",
    "    inc_lst = overall_behavior_array(inc)\n",
    "    dec_lst = overall_behavior_array(dec)\n",
    "    flat_lst = overall_behavior_array(flat)\n",
    "    return inc_lst, dec_lst, flat_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inc_n, dec_n, flat_n = plotable_arrays(inc_n_dict, dec_n_dict, flat_n_dict)\n",
    "# inc, dec, flat = plotable_arrays(inc_dict, dec_dict, flat_dict)\n",
    "inc, dec, flat = plotable_arrays(inct, dect, flatt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig1,ax1 = plot_trace(dec, pre_window, post_window, stim_time)\n",
    "# plot_trace(dec_n, pre_window, post_window, stim_time, ax=ax1)\n",
    "# print(np.shape(dec_n))\n",
    "fig2,ax2 = plot_trace(inc, pre_window, post_window, stim_time)\n",
    "# plot_trace(inc_n, pre_window, post_window, stim_time, ax=ax2)\n",
    "# print(np.shape(inc_n))\n",
    "fig3,ax3 = plot_trace(flat, pre_window, post_window, stim_time)\n",
    "# plot_trace(flat_n, pre_window, post_window, stim_time, ax=ax3)\n",
    "# print(np.shape(flat_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_event_times_list(trials, event_times, ms=True):\n",
    "    trials_idx = np.where(trials)\n",
    "    trials_starts =[]\n",
    "    for idx in trials_idx[0]:\n",
    "        trials_starts.append(event_times[idx])\n",
    "    if ms==True:\n",
    "        trials_starts = [n*10 for n in trials_starts]\n",
    "    print(np.shape(trials_starts))\n",
    "    return trials_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_dict['208'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inc_dict['208'].keys())\n",
    "dec_n_dict.keys() \n",
    "flat_n_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_event_times_dict(total_data_dict, ms=True, total=False, traces=None):\n",
    "    event_starts={}\n",
    "    starts_size=0\n",
    "\n",
    "    for key in total_data_dict:\n",
    "        if total==False:\n",
    "            event_times = total_data_dict[key]['event_times']\n",
    "            idx = traces[key]['idx'][0]\n",
    "            event_times = [event_times[i] for i in idx]\n",
    "            starts_size+=np.size(event_times)\n",
    "            if ms==True:\n",
    "                event_times =  [n*10 for n in event_times]\n",
    "        elif total==True:\n",
    "            event_times=total_data_dict[key]['event_times'][:196]#this is shittily hardcoded but rn it's the smallest number of trials\n",
    "            starts_size+=np.size(event_times)\n",
    "            if ms==True:\n",
    "                event_times =  [n*10 for n in event_times]\n",
    "        event_starts[key] = event_times\n",
    "    print(starts_size)\n",
    "    return event_starts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decrease_starts_ms = get_event_times_dict(total_data_dict, ms=False, total=False, traces=dect)\n",
    "increase_starts_ms = get_event_times_dict(total_data_dict, ms=False, total=False, traces=inct)\n",
    "flat_starts_ms = get_event_times_dict(total_data_dict, ms=False, total=False, traces=flatt)\n",
    "total_starts_ms = get_event_times_dict(total_data_dict, ms=False, total=True, traces=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dic_shape(dic):\n",
    "    sx=len(dic)\n",
    "    sy=0\n",
    "    for key in dic:\n",
    "        temp = len(dic[key])\n",
    "        print('{} is {}'.format(key,temp))\n",
    "        sy+=temp\n",
    "    return sx,sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,dy = dic_shape(increase_starts_ms)\n",
    "print(x,dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "STA_brains={}\n",
    "for key in total_data_dict:\n",
    "    STA_brains[key]={}\n",
    "    STA_brain_decrease = brainsss.make_STA_brain(neural_signals = total_data_dict[key]['signals'],\n",
    "                                       neural_timestamps = total_data_dict[key]['timestamps'],\n",
    "                                       event_times_list = decrease_starts_ms[key],\n",
    "                                       neural_bins = total_data_dict[key]['bins'])\n",
    "#     print(STA_brain_decrease)\n",
    "    STA_brains[key]['decrease']=STA_brain_decrease\n",
    "\n",
    "    STA_brain_increase = brainsss.make_STA_brain(neural_signals = total_data_dict[key]['signals'],\n",
    "                                       neural_timestamps = total_data_dict[key]['timestamps'],\n",
    "                                       event_times_list = increase_starts_ms[key],\n",
    "                                       neural_bins = total_data_dict[key]['bins'])\n",
    "    STA_brains[key]['increase']=STA_brain_increase\n",
    "\n",
    "    STA_brain_flat = brainsss.make_STA_brain(neural_signals = total_data_dict[key]['signals'],\n",
    "                                       neural_timestamps = total_data_dict[key]['timestamps'],\n",
    "                                       event_times_list = flat_starts_ms[key],\n",
    "                                       neural_bins = total_data_dict[key]['bins'])\n",
    "    STA_brains[key]['flat']=STA_brain_flat\n",
    "\n",
    "    STA_brain = brainsss.make_STA_brain(neural_signals = total_data_dict[key]['signals'],\n",
    "                                       neural_timestamps = total_data_dict[key]['timestamps'],\n",
    "                                       event_times_list = total_starts_ms[key],\n",
    "                                       neural_bins = total_data_dict[key]['bins'])\n",
    "    STA_brains[key]['total']=STA_brain\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STA_brains.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STA_brains['208'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for key in STA_brains:\n",
    "    for d in STA_brains[key]:\n",
    "        no_nan = np.nan_to_num(STA_brains[key][d])\n",
    "        reformed_STA_brain = brainsss.STA_supervoxel_to_full_res(no_nan, total_data_dict[key]['cluster_labels'])\n",
    "        STA_brains[key][d] = gaussian_filter1d(reformed_STA_brain,sigma=1,axis=1,truncate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed = brainsss.load_fda_meanbrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "atlas = brainsss.load_roi_atlas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explosion_rois = brainsss.load_explosion_groups()\n",
    "all_rois = brainsss.unnest_roi_groups(explosion_rois)\n",
    "roi_masks = brainsss.make_single_roi_masks(all_rois, atlas)\n",
    "roi_contours = brainsss.make_single_roi_contours(roi_masks, atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def warp_STA_brain(STA_brain, fly, fixed, anat_to_mean_type, func_path):\n",
    "    n_tp = STA_brain.shape[1]\n",
    "    moving_resolution = (2.611, 2.611, 5)\n",
    "    ###########################\n",
    "    ### Organize Transforms ###\n",
    "    ###########################\n",
    "    warp_directory = os.path.join(func_path,'warp')\n",
    "    warp_sub_dir = 'func-to-anat_fwdtransforms_2umiso'\n",
    "    affine_file = os.listdir(os.path.join(warp_directory, warp_sub_dir))[0]\n",
    "    affine_path = os.path.join(warp_directory, warp_sub_dir, affine_file)\n",
    "    if anat_to_mean_type == 'myr':\n",
    "        warp_sub_dir = 'anat-to-meanbrain_fwdtransforms_2umiso'\n",
    "    elif anat_to_mean_type == 'non_myr':\n",
    "        warp_sub_dir = 'anat-to-non_myr_mean_fwdtransforms_2umiso'\n",
    "    else:\n",
    "        print('invalid anat_to_mean_type')\n",
    "        return\n",
    "    syn_files = os.listdir(os.path.join(warp_directory, warp_sub_dir))\n",
    "    syn_linear_path = os.path.join(warp_directory, warp_sub_dir, [x for x in syn_files if '.mat' in x][0])\n",
    "    syn_nonlinear_path = os.path.join(warp_directory, warp_sub_dir, [x for x in syn_files if '.nii.gz' in x][0])\n",
    "    ####transforms = [affine_path, syn_linear_path, syn_nonlinear_path]\n",
    "    transforms = [syn_nonlinear_path, syn_linear_path, affine_path] ### INVERTED ORDER ON 20220503!!!!\n",
    "    #ANTS DOCS ARE SHIT. THIS IS PROBABLY CORRECT, AT LEAST IT NOW WORKS FOR THE FLY(134) THAT WAS FAILING\n",
    "\n",
    "\n",
    "    ### Warp timeponts\n",
    "    warps = []\n",
    "    for tp in range(n_tp):\n",
    "        to_warp = np.rollaxis(STA_brain[:,tp,:,:],0,3)\n",
    "        moving = ants.from_numpy(to_warp)\n",
    "        moving.set_spacing(moving_resolution)\n",
    "        ########################\n",
    "        ### Apply Transforms ###\n",
    "        ########################\n",
    "        moco = ants.apply_transforms(fixed, moving, transforms)\n",
    "        warped = moco.numpy()\n",
    "        warps.append(warped)\n",
    "\n",
    "    return warps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "warp_path = os.path.join(later_dir, 'total_warps.pkl')\n",
    "if os.path.exists(warp_path)==False:\n",
    "    print(\"Making total warps\")\n",
    "    total_warps = {}\n",
    "\n",
    "    for key in STA_brains:\n",
    "        total_warps[key]={}\n",
    "        for d in STA_brains[key]:\n",
    "            fly_name = 'fly_{}'.format(key)\n",
    "            path = f'/oak/stanford/groups/trc/data/Ilana/2P/data/{fly_name}/'\n",
    "            warps = warp_STA_brain(STA_brain=STA_brains[key][d], fly=key, fixed=fixed, anat_to_mean_type='myr', func_path=path)\n",
    "    #             print(warps)\n",
    "            total_warps[key][d] = warps\n",
    "    with open(warp_path, 'wb') as file:\n",
    "        pickle.dump(total_warps, file)\n",
    "else:\n",
    "    print(\"Loading total warps\")\n",
    "    with open(warp_path, 'rb') as file:\n",
    "        total_warps = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_warps['208'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(total_warps['208']['total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_total=[]\n",
    "for fly in total_warps:\n",
    "#     print(fly)\n",
    "    super_vox=[np.ravel(total_warps[fly]['total'][i]) for i in range(np.shape(total_warps['208']['total'])[0])]\n",
    "    super_total.append(super_vox)\n",
    "super_total=np.array(super_total)\n",
    "super_mean=np.mean(super_total,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(super_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(super_mean[:,499910])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(super_mean[:,499900:500000].T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_warps = []\n",
    "inc_warps = []\n",
    "flat_warps = []\n",
    "total = []\n",
    "\n",
    "for fly in total_warps:\n",
    "    for key in total_warps[fly]:\n",
    "        if key=='decrease':\n",
    "#             print(np.shape(total_warps[fly][key]))\n",
    "            dec_warps.append(total_warps[fly][key])\n",
    "        elif key=='increase':\n",
    "#             print(np.shape(total_warps[fly][key]))\n",
    "            inc_warps.append(total_warps[fly][key])\n",
    "        elif key=='flat':\n",
    "#             print(np.shape(total_warps[fly][key]))\n",
    "            flat_warps.append(total_warps[fly][key])\n",
    "        else: \n",
    "#             print(np.shape(total_warps[fly][key]))\n",
    "            total.append(total_warps[fly][key])\n",
    "dec_warp_mean = np.asarray(np.mean(dec_warps, axis=0))\n",
    "inc_warp_mean = np.asarray(np.mean(inc_warps, axis=0))\n",
    "flat_warp_mean = np.asarray(np.mean(flat_warps, axis=0))\n",
    "total_warp_mean = np.asarray(np.mean(total, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_roi_signal_traces(roi_ids, roi_masks, warps, hemi, signal_type):\n",
    "    t0 = time.time()\n",
    "    roi_time_avgs = []\n",
    "    for roi in roi_ids[hemi]:\n",
    "        mask = roi_masks[roi]\n",
    "        masked_data = warps[:,:,:,:]*mask[np.newaxis,:,:,:] #note z-flip\n",
    "        if signal_type == 'max':\n",
    "            roi_time_avg = np.max(masked_data,axis=(1,2,3))\n",
    "        elif signal_type == 'mean':\n",
    "            roi_time_avg = np.mean(masked_data,axis=(1,2,3))\n",
    "        roi_time_avgs.append(roi_time_avg)\n",
    "    print(time.time()-t0)\n",
    "    return np.asarray(roi_time_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "###CHANGE THIS###\n",
    "#################\n",
    "warps = total_warp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vio_dataset={}\n",
    "for i in range(np.shape(warps)[0]):\n",
    "    vio_dataset[i+1]={}\n",
    "    max_val = np.max(total_warp_mean[i,...])\n",
    "    min_val = np.min(total_warp_mean[i,...])\n",
    "    mean_val = np.mean(total_warp_mean[i,...])\n",
    "    vio_dataset[i+1]['max']=max_val\n",
    "    vio_dataset[i+1]['min']=min_val\n",
    "    vio_dataset[i+1]['mean']=mean_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vio_dataset[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warps_2d = []\n",
    "for x in range(np.shape(warps)[0]):\n",
    "    warp_temp = warps[i][...,::-1]\n",
    "    maxs = np.max(warp_temp,axis=2)\n",
    "    mins = np.min(warp_temp,axis=2)\n",
    "    maxs[np.where(np.abs(mins)>maxs)] = mins[np.where(np.abs(mins)>maxs)]\n",
    "#     print(np.shape(maxs))\n",
    "#     maxs=maxs[maxs!=0]\n",
    "#     print(np.shape(maxs))\n",
    "#     flatter = np.ravel(maxs)\n",
    "#     print(np.shape(flatter))\n",
    "    warps_2d.append(maxs)\n",
    "warps_2d = np.asarray(warps_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(warps_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explosions = []\n",
    "t0 = time.time()\n",
    "for tp in range(24):\n",
    "    input_canvas = np.ones((500,500,3)) #+.5 #.5 for diverging\n",
    "    data_to_plot = warps[tp][:,:,::-1]\n",
    "    vmax = 0.5 #this was 0.5 for STA <------------\n",
    "    explosion_map = brainsss.place_roi_groups_on_canvas(explosion_rois,\n",
    "                                                        roi_masks,\n",
    "                                                        roi_contours,\n",
    "                                                        data_to_plot,\n",
    "                                                        input_canvas,\n",
    "                                                        vmax=vmax,\n",
    "                                                        cmap='seismic', diverging=True)#'hot')\n",
    "    explosions.append(explosion_map)\n",
    "print(F'Explosion {time.time()-t0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = 11\n",
    "plt.imshow(explosions[frame][170:,:])\n",
    "print(np.count_nonzero(warps[frame]))\n",
    "print(np.shape(np.ravel(warps[frame])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lesst=[]\n",
    "moret=[]\n",
    "for i in range(np.shape(warps)[0]):\n",
    "    less = np.shape(warps[i][np.where(warps[i]<0)])[0]/np.shape(np.ravel(warps[i]))[0]*100\n",
    "    lesst.append(less)\n",
    "    more = np.shape(warps[i][np.where(warps[i]>0)])[0]/np.shape(np.ravel(warps[i]))[0]*100\n",
    "    moret.append(more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "framee=23\n",
    "print('less percent is {}'.format(lesst[framee]))\n",
    "print('more percent is {}'.format(moret[framee]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "tp =np.arange(np.shape(warps)[0])\n",
    "lesstn = [-x for x in lesst]\n",
    "# ax.axhline(5,color='k',linestyle='--',lw=2)\n",
    "# ax.axhline(15,color='k',linestyle='--',lw=2)\n",
    "ax.barh(tp, lesstn, color='blue')\n",
    "ax.barh(tp, moret, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rois(explosion_rois, roi_group, roi_masks, data_to_plot, roi_contours, vmax, cmap, diverging, roi=None):\n",
    "    x_shift = explosion_rois[roi_group]['x_shift']\n",
    "    y_shift = explosion_rois[roi_group]['y_shift']\n",
    "\n",
    "    roi_data = []\n",
    "    left_edges = []\n",
    "    right_edges = []\n",
    "    bottom_edges = []\n",
    "    top_edges = []\n",
    "\n",
    "    if roi==None:\n",
    "        for roi in explosion_rois[roi_group]['rois']:\n",
    "            mask = roi_masks[roi]\n",
    "            #masked_roi = mask[...,np.newaxis]*data_to_plot # for 3 channel\n",
    "            masked_roi = mask*data_to_plot\n",
    "\n",
    "            ### maximum projection along z-axis\n",
    "            # works for negative values\n",
    "            maxs = np.max(masked_roi,axis=2)\n",
    "            mins = np.min(masked_roi,axis=2)\n",
    "            maxs[np.where(np.abs(mins)>maxs)] = mins[np.where(np.abs(mins)>maxs)]\n",
    "            roi_data.append(maxs)\n",
    "            #masked_roi_flat = maxs\n",
    "\n",
    "            ### maximum projection along z-axis\n",
    "            #masked_roi_flat = np.max(masked_roi,axis=2)\n",
    "            #roi_data.append(masked_roi_flat)\n",
    "\n",
    "            left_edges.append(roi_contours[roi]['left_edge'])\n",
    "            right_edges.append(roi_contours[roi]['right_edge'])\n",
    "            top_edges.append(roi_contours[roi]['top_edge'])\n",
    "            bottom_edges.append(roi_contours[roi]['bottom_edge'])\n",
    "    else:\n",
    "        mask = roi_masks[roi]\n",
    "        #masked_roi = mask[...,np.newaxis]*data_to_plot # for 3 channel\n",
    "        masked_roi = mask*data_to_plot\n",
    "\n",
    "        ### maximum projection along z-axis\n",
    "        # works for negative values\n",
    "        maxs = np.max(masked_roi,axis=2)\n",
    "        mins = np.min(masked_roi,axis=2)\n",
    "        maxs[np.where(np.abs(mins)>maxs)] = mins[np.where(np.abs(mins)>maxs)]\n",
    "        roi_data.append(maxs)\n",
    "        #masked_roi_flat = maxs\n",
    "\n",
    "        ### maximum projection along z-axis\n",
    "        #masked_roi_flat = np.max(masked_roi,axis=2)\n",
    "        #roi_data.append(masked_roi_flat)\n",
    "\n",
    "        left_edges.append(roi_contours[roi]['left_edge'])\n",
    "        right_edges.append(roi_contours[roi]['right_edge'])\n",
    "        top_edges.append(roi_contours[roi]['top_edge'])\n",
    "        bottom_edges.append(roi_contours[roi]['bottom_edge'])\n",
    "\n",
    "\n",
    "    # get extreme edges from all rois used\n",
    "    left_edge = np.min(left_edges) - 1\n",
    "    right_edge = np.max(right_edges) + 1\n",
    "    top_edge = np.min(top_edges) - 1\n",
    "    bottom_edge = np.max(bottom_edges) + 1\n",
    "\n",
    "    ### this projects across all the roi_data from each roi \n",
    "    #roi_datas = np.max(np.asarray(roi_data),axis=0) # this one line is sufficient for not diverging\n",
    "    maxs = np.max(np.asarray(roi_data),axis=0)\n",
    "    mins = np.min(np.asarray(roi_data),axis=0)\n",
    "    maxs[np.where(np.abs(mins)>maxs)] = mins[np.where(np.abs(mins)>maxs)]\n",
    "    roi_datas = maxs\n",
    "\n",
    "    ###ADD MAX MIN HERE LIKE ABOVE\n",
    "\n",
    "    ### cutout this grouping\n",
    "    #data_map = np.swapaxes(roi_datas[top_edge:bottom_edge,left_edge:right_edge,:],0,1) # for 3 channel\n",
    "    data_map = np.swapaxes(roi_datas[top_edge:bottom_edge,left_edge:right_edge],0,1)\n",
    "    ### apply gain\n",
    "    #data_map = data_map * gain\n",
    "\n",
    "    mycmap = matplotlib.cm.get_cmap(cmap)\n",
    "    #mycmap.set_bad('k',1) # make nans black\n",
    "\n",
    "    if diverging:\n",
    "        # this will normalize all value to [0,1], with 0.5 being the new \"0\" basically\n",
    "        # current issue - a zero that should be background now looks like negative.\n",
    "        # solution: could use nans instead and set bad color\n",
    "        # with diverging we should make background white!\n",
    "        # so actually just set the input_canvas as 0.5!!!\n",
    "        # then make contours nan and set back as black\n",
    "        norm = matplotlib.colors.Normalize(vmin=-vmax, vmax=vmax)\n",
    "        data_map = norm(data_map)\n",
    "    else:\n",
    "        data_map = data_map/vmax\n",
    "\n",
    "    data_map = mycmap(data_map)[...,:3] #lose alpha channel\n",
    "\n",
    "#     dims = get_dim_info(data_map, full_x_mid, full_y_mid)\n",
    "    return data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explosion_rois.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "explosion_rois['PB']['rois']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_specific_anat(group, roi=None):\n",
    "    vmax = 0.5 #this was 0.5 for STA <------------\n",
    "    area_tot = []\n",
    "    for j in range(np.shape(warps)[0]):\n",
    "        data_map = get_rois(explosion_rois, group, roi_masks, warps[j][:,:,::-1], roi_contours, vmax=vmax, cmap='seismic', diverging=True, roi=roi)\n",
    "        area_tot.append(data_map)\n",
    "    area_tot = np.asarray(area_tot)\n",
    "    return area_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anat_tot = get_specific_anat('PB')\n",
    "anat_tot2 = get_specific_anat('PB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tipo = 18\n",
    "area = anat_tot\n",
    "print(np.shape(area[tipo,...]))\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "ax.imshow(area[tipo,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_avg_single(array):\n",
    "    vals = [np.ravel(array[x]) for x in range(np.shape(array)[0]) ]\n",
    "    vals_mean =np.mean(vals, axis=1)\n",
    "#     print(np.shape(vals))\n",
    "#     print(np.shape(vals_mean))\n",
    "    return vals_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vals_mean1=get_avg_single(anat_tot)\n",
    "vals_mean2=get_avg_single(anat_tot2)\n",
    "two = [vals_mean1, vals_mean2]\n",
    "print(np.shape(two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "names=[]\n",
    "avgs={}\n",
    "for group in explosion_rois:\n",
    "#     print(group)\n",
    "    for roi in explosion_rois[group]['rois']:\n",
    "#         print(explosion_rois[group]['rois'][roi])\n",
    "#         print(explosion_rois[group]['rois'][roi][-2])\n",
    "        if explosion_rois[group]['rois'][roi][-2]=='_':\n",
    "            area_name=explosion_rois[group]['rois'][roi][:-2]\n",
    "#             print(area_name)\n",
    "        else:\n",
    "            area_name=explosion_rois[group]['rois'][roi]\n",
    "#             print(area_name)\n",
    "        if area_name not in names:\n",
    "            names.append(area_name)\n",
    "        one_area=get_specific_anat(group=group,roi=roi)\n",
    "        avg_area=get_avg_single(one_area)\n",
    "        if area_name not in avgs:\n",
    "            avgs[area_name]=avg_area\n",
    "        else:\n",
    "            avgs[area_name]=np.mean([avgs[area_name],avg_area], axis=0)\n",
    "#             print(np.shape(avgs[area_name]))\n",
    "#             avgs[area_name]=avg_area\n",
    "print(np.shape(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total=[]\n",
    "for key in avgs:\n",
    "    total.append(avgs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.min(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(40,10))\n",
    "im = ax.imshow(total, cmap='seismic_r', vmin=0.8, vmax=1.04)\n",
    "ax.set_yticks(np.arange(np.shape(total)[0]))\n",
    "ax.set_yticklabels(names);\n",
    "fig.tight_layout()\n",
    "# ax.tick_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_dir = os.path.join(later_dir, 'movies')\n",
    "if os.path.exists(movie_dir)==False:\n",
    "    os.mkdir(movie_dir)\n",
    "print(movie_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_dir = os.path.join(movie_dir,'decrease')\n",
    "if os.path.exists(save_dir)==False:\n",
    "    os.mkdir(save_dir)\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(24):\n",
    "    print(i)\n",
    "    plt.imshow(explosions[i][170:,:]) #this was made with cmap=hot\n",
    "    fname = os.path.join(save_dir, '{0:05d}.png'.format(i))\n",
    "    plt.savefig(fname,dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
