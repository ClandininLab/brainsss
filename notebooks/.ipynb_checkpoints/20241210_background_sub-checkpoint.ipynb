{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brainsss\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.signal import butter, sosfiltfilt, filtfilt, freqz\n",
    "from scipy import signal\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pyplot import cm\n",
    "import random\n",
    "from scipy.stats import sem\n",
    "import time\n",
    "import h5py\n",
    "import ants\n",
    "import nibabel as nib\n",
    "import matplotlib\n",
    "from scipy.ndimage import gaussian_filter1d,gaussian_filter\n",
    "import pickle\n",
    "from skimage import io, filters\n",
    "import glob\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.image import grid_to_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fly_num = 'fly_208'\n",
    "func_path = f'/oak/stanford/groups/trc/data/Ilana/2P/data/{fly_num}/'\n",
    "# load_directory = os.path.join(func_path, 'func_0/background_subtraction/functional_channel_2_moco.h5')\n",
    "load_directory = os.path.join(func_path, 'func_0/moco/functional_channel_2_moco.h5')\n",
    "# save_directory = os.path.join(func_path, 'func_0/background_subtraction/functional_channel_2_moco_highpass_test.h5')\n",
    "# load_directory = os.path.join(func_path, 'func_0/functional_channel_2_moco_zscore.h5')\n",
    "warp_directory = os.path.join(func_path,'warp')\n",
    "load_dir = os.path.join(warp_directory,  'test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with h5py.File(load_directory, 'r') as wf:\n",
    "        data = wf['data'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(data[...,20,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(load_directory2, 'r') as wf:\n",
    "        data2 = wf['data']\n",
    "        print(np.max(data[...,20,1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    return butter(order, cutoff, fs=fs, btype='high', analog=False)\n",
    "\n",
    "def butter_highpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_highpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "def apply_butter_highpass(data, z, cutoff, order, fs):\n",
    "\n",
    "    # Get the filter coefficients so we can check its frequency response.\n",
    "    b, a = butter_highpass(cutoff, fs, order)\n",
    "    hpf_data = butter_highpass_filter(data[:,:,z, :], cutoff, fs, order)\n",
    "    return hpf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter requirements \n",
    "order = 2     # ?? wtf is this??\n",
    "fs = 1.8      # sample rate, Hz\n",
    "cutoff = 0.01  # desired cutoff frequency of the filter, Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#from warps\n",
    "\n",
    "with h5py.File(load_dir, 'r') as wf:\n",
    "        warps = wf['data'][:]\n",
    "        ts=wf['timestamps'][:]\n",
    "        dims = np.shape(warps)\n",
    "        dimsts = np.shape(ts)\n",
    "        print(\"Data shape is {} and TimeStamps shape is {}\".format(dims, dimsts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# warps = np.rollaxis(warps,0,4)\n",
    "print(np.shape(warps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(np.max(warps[:,:,20,:],axis=-1).T)\n",
    "plt.subplot(2,1,2)\n",
    "plt.imshow(np.max(ts[:,:,20,:],axis=-1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "warps_blur=[]\n",
    "for i in range(np.shape(warps)[-1]):\n",
    "    warps_temp = gaussian_filter(warps[...,i], sigma=2)\n",
    "    warps_blur.append(warps_temp)\n",
    "warps_blur=np.asarray(warps_blur)\n",
    "\n",
    "print(np.shape(warps_blur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warps_blur=np.moveaxis(warps_blur,0,-1)\n",
    "np.shape(warps_blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.max(warps_blur[:,:,20,:], axis=-1).T)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.max(warps[:,:,20,:],axis=-1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#CHANGE THIS#\n",
    "#############\n",
    "\n",
    "warp_def=warps_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(warp_def)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stepsize=100\n",
    "dims=np.shape(warp_def)\n",
    "steps = list(range(0,dims[-1],stepsize))\n",
    "steps.append(dims[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hpf_total = np.zeros(dims)\n",
    "for z in range(dims[-2]):\n",
    "    for chunk_num in steps:\n",
    "        cs=chunk_num\n",
    "        ce=chunk_num+stepsize\n",
    "        if ce <= steps[-1]:\n",
    "#             print(ce)\n",
    "            hpf_warps = apply_butter_highpass(warp_def[...,cs:ce], z, cutoff, order, fs)\n",
    "#             print(np.shape(hpf_warps))\n",
    "            hpf_total[...,z,cs:ce]=hpf_warps\n",
    "#             print(\"z={}, t={}:{}\".format(z,cs,ce))\n",
    "hpf_total = np.array(hpf_total)\n",
    "# hpf_total = np.transpose(hpf_total, (1,2,0,3))\n",
    "dims_hpfw = np.shape(hpf_total)\n",
    "print(\"High Pass Filter Data shape is {}\".format(dims_hpfw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(hpf_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpf_total = warp_def-hpf_total\n",
    "print(np.shape(lpf_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed = brainsss.load_fda_meanbrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(fixed[...,50].T)\n",
    "plt.colorbar()\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(data[...,50].T)\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dff=hpf_total/(lpf_total-lpf_total.min()+100)\n",
    "print(dff.mean())\n",
    "dff=np.where(fixed.numpy()[...,None]>0.1, dff, 0)\n",
    "print(dff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.max(hpf_total[:,:,20, :], axis=-1).T)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.max(lpf_total[:,:,20, :], axis=-1).T)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.max(dff[:,:,20, :], axis=-1).T, vmax=1,vmin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain=dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape=np.shape(brain)\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#make supervoxels\n",
    "cluster_dir = os.path.join(func_path, 'clustering')\n",
    "if not os.path.exists(cluster_dir):\n",
    "    os.mkdir(cluster_dir)\n",
    "\n",
    "n_clusters = 2000\n",
    "connectivity = grid_to_graph(shape[0],shape[1])\n",
    "cluster_labels = []\n",
    "t_shape = shape[3]\n",
    "for z in range(shape[-2]): \n",
    "    neural_activity = brain[:,:,z,:].reshape(-1, t_shape)\n",
    "    cluster_model = AgglomerativeClustering(n_clusters=n_clusters,\n",
    "                                memory=cluster_dir,\n",
    "                                linkage='ward',\n",
    "                                connectivity=connectivity)\n",
    "    cluster_model.fit(neural_activity)\n",
    "    cluster_labels.append(cluster_model.labels_)\n",
    "cluster_labels = np.asarray(cluster_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "### GET CLUSTER AVERAGE SIGNAL ###\n",
    "all_signals = []\n",
    "for z in range(shape[-2]):\n",
    "    neural_activity = brain[:,:,z,:].reshape(-1, t_shape)\n",
    "    signals = []\n",
    "    for cluster_num in range(n_clusters):\n",
    "        cluster_indicies = np.where(cluster_labels[z,:]==cluster_num)[0]\n",
    "        mean_signal = np.mean(neural_activity[cluster_indicies,:], axis=0)\n",
    "        signals.append(mean_signal)\n",
    "    signals = np.asarray(signals)\n",
    "    all_signals.append(signals)\n",
    "all_signals = np.asarray(all_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(all_signals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "### PREP VISUAL STIMULI ###\n",
    "###########################\n",
    "\n",
    "vision_path = os.path.join(func_path,'func_0', 'visual')\n",
    "\n",
    "### Load Photodiode ###\n",
    "t, ft_triggers, pd1, pd2 = brainsss.load_photodiode(vision_path)\n",
    "stimulus_start_times = brainsss.extract_stim_times_from_pd(pd2, t)\n",
    "\n",
    "# *100 puts in units of 10ms, which will match fictrac\n",
    "st_10ms = [int(stimulus_start_times[i]*100) for i in range(len(stimulus_start_times))]\n",
    "\n",
    "# get 1ms version to match neural timestamps\n",
    "st_ms= [i*10 for i in st_10ms]\n",
    "starts_loom = st_10ms\n",
    "\n",
    "####################\n",
    "### Prep Fictrac ###\n",
    "####################\n",
    "\n",
    "fictrac_path = os.path.join(func_path, 'func_0', 'fictrac')\n",
    "fictrac_raw = brainsss.load_fictrac(fictrac_path)\n",
    "\n",
    "fps = 100\n",
    "resolution = 10 #desired resolution in ms\n",
    "expt_len = fictrac_raw.shape[0]/fps*1000\n",
    "behaviors = ['dRotLabY', 'dRotLabZ', 'dRotLabX', 'speed']\n",
    "fictrac = {}\n",
    "for behavior in behaviors:\n",
    "    if behavior == 'dRotLabY': short = 'Y'\n",
    "    elif behavior == 'dRotLabZ': short = 'Z'\n",
    "    elif behavior == 'dRotLabX': short = 'X'\n",
    "    fictrac[short] = brainsss.smooth_and_interp_fictrac(fictrac_raw, fps, resolution, expt_len, behavior)\n",
    "fictrac_timestamps = np.arange(0,expt_len,resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_traces(fictrac, stim_times, pre_window, post_window, val=None):\n",
    "    traces = []\n",
    "    for i in range(len(stim_times)):\n",
    "        if val != None:\n",
    "            trace = fictrac[val][stim_times[i]-pre_window:stim_times[i]+post_window]\n",
    "        else:\n",
    "            trace = fictrac[stim_times[i]-pre_window:stim_times[i]+post_window]\n",
    "        if len(trace) == pre_window + post_window: # this handles fictrac that crashed or was aborted or some bullshit\n",
    "            traces.append(trace)\n",
    "    traces = np.asarray(traces)\n",
    "    mean_trace = np.mean(traces,axis=0)\n",
    "    sem_trace = scipy.stats.sem(traces,axis=0)\n",
    "    return traces, mean_trace, sem_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Extract Stimulus Triggered Behavior ###\n",
    "###########################################\n",
    "\n",
    "pre_window = 200\n",
    "post_window = 300\n",
    "# avg_around = 20\n",
    "stim_time = 100\n",
    "# window = np.arange(-pre_window,post_window)\n",
    "\n",
    "behavior_traces = {}\n",
    "mean_trace = {}\n",
    "sem_trace = {}\n",
    "behavior_traces,mean_trace,sem_trace = extract_traces(fictrac['Y'], starts_loom, pre_window, post_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_trace(behavior_trace, pre_window, post_window, stim_time):\n",
    "    mean_trace = np.mean(behavior_trace, axis=0)\n",
    "    sem_trace = scipy.stats.sem(behavior_trace, axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(mean_trace,color='k',linewidth=3)\n",
    "    plt.fill_between(np.arange(len(mean_trace)),mean_trace-sem_trace, mean_trace+sem_trace, color='k',alpha=0.3)\n",
    "    plt.axvline(pre_window,color='k',linestyle='--',lw=2)\n",
    "    plt.axvline(pre_window+stim_time,color='k',linestyle='--',lw=2)\n",
    "    plt.ylim(-0.5, 3.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(behavior_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_trace(behavior_traces, pre_window, post_window, stim_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starts_loom_ms=[n*10 for n in starts_loom]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_start = -500; bin_end = 2000; bin_size = 100 #ms\n",
    "neural_bins = np.arange(bin_start,bin_end,bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestamps = brainsss.load_timestamps(os.path.join(func_path,'func_0', 'imaging'))\n",
    "print(np.shape(timestamps))\n",
    "print(np.shape(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_STA_brain(neural_signals, neural_timestamps, event_times_list, neural_bins):\n",
    "    #### super voxel version\n",
    "    shape=np.shape(neural_signals)\n",
    "    STA_brain = []\n",
    "    for z in range(shape[0]):\n",
    "        all_bin_indicies = []\n",
    "        for stim_idx in range(len(event_times_list)):\n",
    "            stim_time = event_times_list[stim_idx]\n",
    "            stim_centered_bins = neural_bins + stim_time\n",
    "            bin_indicies = np.digitize(neural_timestamps[...,z,:] , stim_centered_bins)\n",
    "            all_bin_indicies.append(bin_indicies)\n",
    "        all_bin_indicies = np.asarray(all_bin_indicies)\n",
    "\n",
    "        avg_neural_across_bins = []\n",
    "        for bin_num in np.arange(1,len(neural_bins)):\n",
    "            this_bin_sample_times = list(np.where(all_bin_indicies==bin_num)[1])\n",
    "            average_neural_in_bin = np.mean(neural_signals[z,:,this_bin_sample_times],axis=0)\n",
    "            avg_neural_across_bins.append(average_neural_in_bin)\n",
    "        avg_neural_across_bins = np.asarray(avg_neural_across_bins)\n",
    "        STA_brain.append(avg_neural_across_bins)\n",
    "    STA_brain = np.asarray(STA_brain)\n",
    "    return STA_brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "STA_brain=make_STA_brain(neural_signals=all_signals, neural_timestamps=ts, \n",
    "                         event_times_list=starts_loom_ms, neural_bins=neural_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z=20\n",
    "neural_signals=all_signals \n",
    "neural_timestamps=ts \n",
    "event_times_list=starts_loom_ms\n",
    "neural_bins=neural_bins\n",
    "\n",
    "shape=np.shape(neural_signals)\n",
    "STA_brain = []\n",
    "\n",
    "all_bin_indicies = []\n",
    "for stim_idx in range(len(event_times_list)):\n",
    "    stim_time = event_times_list[stim_idx]\n",
    "    stim_centered_bins = neural_bins + stim_time\n",
    "    bin_indicies = np.digitize(neural_timestamps[...,z,:] , stim_centered_bins)\n",
    "    print(np.shape(bin_indicies))\n",
    "#     all_bin_indicies.append(bin_indicies)\n",
    "# all_bin_indicies = np.asarray(all_bin_indicies)\n",
    "\n",
    "# avg_neural_across_bins = []\n",
    "# for bin_num in np.arange(1,len(neural_bins)):\n",
    "#     this_bin_sample_times = list(np.where(all_bin_indicies==bin_num)[1])\n",
    "#     average_neural_in_bin = np.mean(neural_signals[z,:,this_bin_sample_times],axis=0)\n",
    "#     avg_neural_across_bins.append(average_neural_in_bin)\n",
    "# avg_neural_across_bins = np.asarray(avg_neural_across_bins)\n",
    "# STA_brain.append(avg_neural_across_bins)\n",
    "\n",
    "# STA_brain = np.asarray(STA_brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bool_starts=(starts_loom_ms>=(np.min(ts))) & (starts_loom_ms<=(np.max(ts)))\n",
    "starts_loom_ms=np.array(starts_loom_ms)\n",
    "starts_loom_ms=starts_loom_ms[bool_starts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins_array=[]\n",
    "for loom in starts_loom_ms:\n",
    "#     print(loom)\n",
    "    start=loom+bin_start\n",
    "    end=loom+bin_end-bin_size\n",
    "#     edges=[start,end]\n",
    "    bins_array.append(start)\n",
    "    bins_array.append(end)\n",
    "# bins_test=np.vstack(bins_test)\n",
    "bins_array=np.array(bins_array)\n",
    "np.shape(bins_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(brain))\n",
    "print(len(starts_loom_ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.shape(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "bin_idx = np.digitize(ts, bins_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# make loom-relative version of ts\n",
    "ts_rel = ts.copy()\n",
    "\n",
    "# Loop through each loom-containing bin_idx and subtract starts_loom_ms\n",
    "for i in range(len(starts_loom_ms)):\n",
    "    # subtract loom onset time for corresponding timestamps\n",
    "    ts_rel[bin_idx == i*2 + 1] -= starts_loom_ms[i]\n",
    "\n",
    "# boolean mask of where bin_idx is odd\n",
    "odd_mask = bin_idx % 2 == 1\n",
    "\n",
    "# Create flattened (xyz X time) \n",
    "n_timesteps = ts_rel.shape[-1]\n",
    "ts_rel_flat = ts_rel.reshape(-1, n_timesteps)\n",
    "brain_flat = brain.reshape(-1, n_timesteps)\n",
    "odd_mask_flat = odd_mask.reshape(-1, n_timesteps)\n",
    "\n",
    "# Collect ts_rel and brain elements that fall within loom window / bin\n",
    "within_bin_brain_flat  = [brain_flat[xyz][odd_mask_flat[xyz]] for xyz in range(brain_flat.shape[0])]\n",
    "within_bin_ts_rel_flat = [ts_rel_flat[xyz][odd_mask_flat[xyz]] for xyz in range(ts_rel_flat.shape[0])]\n",
    "\n",
    "# Find the maximum length of the sublists\n",
    "max_len = max(len(sublist) for sublist in within_bin_brain_flat)\n",
    "\n",
    "# Create a 2D NumPy array filled with np.nan, with the appropriate shape\n",
    "n_voxels = len(within_bin_brain_flat)\n",
    "within_bin_brain_flat_np = np.full((n_voxels, max_len), np.nan)\n",
    "within_bin_ts_rel_flat_np = np.full((n_voxels, max_len), np.nan)\n",
    "\n",
    "# Populate the array with the values from the original list of lists\n",
    "for i, (brain_sl, ts_rel_sl) in enumerate(zip(within_bin_brain_flat, within_bin_ts_rel_flat)):\n",
    "    within_bin_brain_flat_np[i, :len(brain_sl)] = brain_sl\n",
    "    within_bin_ts_rel_flat_np[i, :len(ts_rel_sl)] = ts_rel_sl\n",
    "\n",
    "# unflatten\n",
    "static_brain_shape = brain.shape[:-1]\n",
    "within_bin_brain_np = within_bin_brain_flat_np.reshape(*static_brain_shape, max_len)\n",
    "within_bin_ts_rel_np = within_bin_ts_rel_flat_np.reshape(*static_brain_shape, max_len)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.nanmax(within_bin_brain_np[:,:,20], axis=-1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "within_bin_brain_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "within_bin_ts_rel_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(within_bin_ts_rel_np[100,50,20], within_bin_brain_np[100,50,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dff_dir = os.path.join(func_path, 'dff')\n",
    "if os.path.exists(dff_dir)==False:\n",
    "    os.mkdir(dff_dir)\n",
    "print(dff_dir)\n",
    "dff_path = os.path.join(dff_dir,  'test_dff.h5')\n",
    "with h5py.File(dff_path, \"w\") as data_file:\n",
    "    data_file.create_dataset(\"data\", data=within_bin_brain_np.astype('float32'))\n",
    "    data_file.create_dataset(\"timestamps\", data=within_bin_ts_rel_np.astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dff_dir = os.path.join(func_path, 'dff')\n",
    "dff_path = os.path.join(dff_dir,  'test_dff.h5')\n",
    "with h5py.File(dff_path, 'r') as hf:\n",
    "        brain = hf['data'][:]\n",
    "        ts = hf['timestamps'][:]\n",
    "        dimsw = np.shape(brain)\n",
    "        dimsts = np.shape(ts)\n",
    "        print(\"Data shape is {} and timestamps shape is {}\".format(dimsw, dimsts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.nanmax(brain[:,:,20], axis=-1).T)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(np.nanmax(ts[:,:,20], axis=-1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts[100,50,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_start = -500; bin_end = 2000; bin_size = 100 #ms\n",
    "neural_bins = np.arange(bin_start,bin_end,bin_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_roi_signal_traces(roi_ids, roi_masks, warps, hemi, signal_type):\n",
    "    t0 = time.time()\n",
    "    roi_time_avgs = []\n",
    "    for roi in roi_ids[hemi]:\n",
    "        mask = roi_masks[roi]\n",
    "        masked_data = warps[:,:,:,:]*mask[np.newaxis,:,:,:] #note z-flip\n",
    "        if signal_type == 'max':\n",
    "            roi_time_avg = np.max(masked_data,axis=(1,2,3))\n",
    "        elif signal_type == 'mean':\n",
    "            roi_time_avg = np.mean(masked_data,axis=(1,2,3))\n",
    "        roi_time_avgs.append(roi_time_avg)\n",
    "    print(time.time()-t0)\n",
    "    return np.asarray(roi_time_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "explosions = []\n",
    "for tp in range(24):\n",
    "    input_canvas = np.ones((500,500,3)) #+.5 #.5 for diverging\n",
    "    data_to_plot = warps[tp][:,:,::-1]\n",
    "    vmax = 0.5 #this was 0.5 for STA <------------\n",
    "    explosion_map = brainsss.place_roi_groups_on_canvas(explosion_rois,\n",
    "                                                        roi_masks,\n",
    "                                                        roi_contours,\n",
    "                                                        data_to_plot,\n",
    "                                                        input_canvas,\n",
    "                                                        vmax=vmax,\n",
    "                                                        cmap='seismic', diverging=True)#'hot')\n",
    "    explosions.append(explosion_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
