{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import scipy\n",
    "import nibabel as nib\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "import brainsss\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stimulus_metadata(vision_path):\n",
    "\n",
    "    ### try to get from pickle ###\n",
    "    pickle_path = os.path.join(vision_path, 'stimulus_metadata.pkl')\n",
    "    if os.path.exists(pickle_path):\n",
    "        print(\"Loaded from Pickle.\")\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        return metadata['stim_ids'], metadata['angles']\n",
    "\n",
    "    ### if no pickle, load from .h5 and save pickle for future ###\n",
    "    print(\"No pickle; parsing visprotocol .h5\")\n",
    "    fname = [x for x in os.listdir(vision_path) if '.hdf5' in x][0]\n",
    "    visprotocol_file = os.path.join(vision_path, fname)\n",
    "\n",
    "    stim_ids = []\n",
    "    angles = []\n",
    "    with h5py.File(visprotocol_file, 'r') as f:\n",
    "\n",
    "        ### loop over flies and series to find the one that has many stim presentations (others were aborted)\n",
    "        # note it is critical each fly has their own .h5 file saved\n",
    "        fly_ids = list(f['Flies'].keys())\n",
    "        print(\"Found fly ids: {}\".format(fly_ids))\n",
    "        for fly_id in fly_ids:\n",
    "\n",
    "            series = list(f['Flies'][fly_id]['epoch_runs'].keys())\n",
    "            print(\"Found series: {}\".format(series))\n",
    "            for serie in series:\n",
    "\n",
    "                epoch_ids = f['Flies'][fly_id]['epoch_runs'][serie].get('epochs').keys()\n",
    "                print(str(len(epoch_ids)))\n",
    "                for i, epoch_id in enumerate(epoch_ids):\n",
    "                    stim_id = f['Flies'][fly_id]['epoch_runs'][serie].get('epochs').get(epoch_id).attrs['component_stim_type']\n",
    "                    stim_ids.append(stim_id)\n",
    "                    if stim_id == 'DriftingSquareGrating':\n",
    "                        angle = f['Flies'][fly_id]['epoch_runs'][serie].get('epochs').get(epoch_id).attrs['angle']\n",
    "                        angles.append(angle)\n",
    "                    else:\n",
    "                        angles.append(None)\n",
    "\n",
    "                if len(stim_ids) > 100:\n",
    "\n",
    "                    ### save pickle for next time\n",
    "                    metadata = {'stim_ids': stim_ids, 'angles': angles}\n",
    "                    save_file = os.path.join(vision_path, 'stimulus_metadata.pkl')\n",
    "                    with open(save_file, 'wb') as f:\n",
    "                        pickle.dump(metadata, f)\n",
    "                    print(\"created {}\".format(save_file))\n",
    "\n",
    "                    return stim_ids, angles\n",
    "        print('Could not get visual metadata.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading photodiode data... done\n",
      "Loaded from Pickle.\n",
      "Found 269 presented stimuli.\n",
      "starts_angle_0: 88. starts_angle_180: 88\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "func_path = '/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset/fly_116/func_0/'\n",
    "###########################\n",
    "### PREP VISUAL STIMULI ###\n",
    "###########################\n",
    "\n",
    "vision_path = os.path.join(func_path, 'visual')\n",
    "\n",
    "### Load Photodiode ###\n",
    "t, ft_triggers, pd1, pd2 = brainsss.load_photodiode(vision_path)\n",
    "stimulus_start_times = brainsss.extract_stim_times_from_pd(pd2, t)\n",
    "\n",
    "### Get Metadata ###\n",
    "stim_ids, angles = get_stimulus_metadata(vision_path)\n",
    "print(F\"Found {len(stim_ids)} presented stimuli.\")\n",
    "\n",
    "# *100 puts in units of 10ms, which will match fictrac\n",
    "starts_angle_0 = [int(stimulus_start_times[i]*100) for i in range(len(stimulus_start_times)) if angles[i] == 0]\n",
    "starts_angle_180 = [int(stimulus_start_times[i]*100) for i in range(len(stimulus_start_times)) if angles[i] == 180]\n",
    "print(F\"starts_angle_0: {len(starts_angle_0)}. starts_angle_180: {len(starts_angle_180)}\")\n",
    "list_in_ms = {'0': [i*10 for i in starts_angle_0],\n",
    "                '180': [i*10 for i in starts_angle_180]}\n",
    "\n",
    "brain_path = os.path.join(func_path, 'functional_channel_2_moco_zscore_highpass.h5')\n",
    "timestamps = brainsss.load_timestamps(os.path.join(func_path, 'imaging'), file='functional.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_slice(brain_path, slice_num):\n",
    "    with h5py.File(brain_path, 'r') as hf:\n",
    "        single_slice = hf['data'][:,:,slice_num,:]\n",
    "    return single_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop over stimuli presentations\n",
    "z = 10\n",
    "bin_start = 2500 #in ms--2.5 seconds before the stimulus is presented\n",
    "bin_end = 5500 #in ms--5.5 seconds after the stimulus is presented\n",
    "iri = timestamps[1,z]-timestamps[0,z] #inter recording interval in ms\n",
    "num_pre_neural_points = int(bin_start/iri)+1\n",
    "num_post_neural_points = int(bin_end/iri)\n",
    "single_slice = load_slice(brain_path, z)\n",
    "data_dic = {'xs0': [],'ys0': [], 'xs180': [], 'ys180': []} #dic that includes neural data and relative time arrays for both stimuli\n",
    "for angle in list_in_ms.keys():\n",
    "    for i in range(len(list_in_ms[angle])):\n",
    "        # and for each presentation use np.searchsorted to find the nearest real neural datapoint.\n",
    "        nearest = np.searchsorted(timestamps[:,z], list_in_ms[angle][i])\n",
    "        #print(f'stim_time:{list_in_ms[angle][i]} & timestamp:{timestamps[nearest,20]}')\n",
    "        offset = timestamps[nearest, z]-list_in_ms[angle][i]\n",
    "        xs = np.arange(offset-num_pre_neural_points*iri, offset+num_post_neural_points*iri, iri)\n",
    "        #grab this datapoint as well as the flanking neural data\n",
    "        ys = single_slice[:, :, nearest-num_pre_neural_points:nearest+num_post_neural_points]\n",
    "        if np.shape(ys)[-1] == len(xs):\n",
    "            if angle == '0':\n",
    "                data_dic['xs0'].append(xs); data_dic['ys0'].append(ys)\n",
    "            else:\n",
    "                data_dic['xs180'].append(xs); data_dic['ys180'].append(ys)\n",
    "xss_0 = np.asarray(data_dic['xs0'])\n",
    "yss_0 = np.asarray(data_dic['ys0'])\n",
    "xss_180 = np.asarray(data_dic['xs180'])\n",
    "yss_180 = np.asarray(data_dic['ys180'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use np.digitize to bin these timepoints and finally average them (bin the neural data and average it)\n",
    "# TODO: figure out why we're getting an error and also why so many of the indicies are empty\n",
    "def bin_avg(bin_size, bin_start, bin_end, xss, yss):\n",
    "    neural_bins = np.arange(-bin_start,bin_end,bin_size)\n",
    "    bin_idxs = np.digitize(xss.ravel(), neural_bins)\n",
    "    yss = np.swapaxes(yss,0,-2)\n",
    "    yss = np.swapaxes(yss,0,1)\n",
    "    dims = np.shape(yss)\n",
    "    yss = np.reshape(yss, (dims[0],dims[1],dims[2]*dims[3]))\n",
    "    avg_by_bin = []\n",
    "    num_in_bin = []\n",
    "    for i in range(len(neural_bins)):\n",
    "        num_in_bin.append(len(np.where(bin_idxs==i)[0]))\n",
    "        if np.size(np.where(bin_idxs==i)[0]) != 0:\n",
    "            avg_by_bin.append(np.mean(yss[:,:, np.where(bin_idxs==i)[0]],axis=2))\n",
    "        else:\n",
    "            avg_by_bin.append(np.nan)\n",
    "    return avg_by_bin, num_in_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg,num = bin_avg(100, bin_start, bin_end, xss_0, yss_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir('/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/figs/20220329_movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(5,len(avg)):\n",
    "#     #print(i)\n",
    "#     plt.imshow(avg[i].T)\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     save_path = '/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/figs/20220329_movie'\n",
    "#     #save_path = '/home/users/yandanw/20220321_movie_beh'\n",
    "#     fname = os.path.join(save_path, '{0:05d}.png'.format(i-5))\n",
    "#     plt.savefig(fname,dpi=100,bbox_inches='tight')\n",
    "#     plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loop over stimuli presentations, \n",
    "# and for each presentation use np.searchsorted (google function to learn about it) to \n",
    "# find the nearest real neural datapoint. Then, grab this datapoint as well as the \n",
    "# flanking neural data and append it to a growing list (below this is yss) along with the time \n",
    "# relative to the stimulus at 0 (below this is xss)\n",
    "def bout_triggered(fly, neural_data, all_bouts, bout_type, original_z):\n",
    "    if bout_type == 'start_bouts':\n",
    "        align_to = 'start'\n",
    "    elif bout_type == 'stop_bouts':\n",
    "        align_to = 'end'\n",
    "    before = 3000 #in ms\n",
    "    after = 3000 # in ms\n",
    "    jump = flies[fly].timestamps[1,0]-flies[fly].timestamps[0,0]\n",
    "    num_neural_points = int(before/jump)\n",
    "\n",
    "    before = int(before/10) # now everything is in units of 10ms\n",
    "    after = int(after/10)\n",
    "    bins = bbb.create_bins(10,before*10,after*10)[:-1]\n",
    "\n",
    "    xss = []; yss = []\n",
    "    for i in range(len(all_bouts[bout_type])):\n",
    "        nearest = np.searchsorted(flies[fly].timestamps[:,original_z]/10, all_bouts[bout_type][i][align_to])\n",
    "        offset = (flies[fly].timestamps[nearest,original_z]/10 - all_bouts[bout_type][i][align_to])*10\n",
    "        xs = np.arange(offset-num_neural_points*jump,offset+num_neural_points*jump,jump)\n",
    "        ys = neural_data[nearest-num_neural_points:nearest+num_neural_points]\n",
    "        if len(ys) == 10:\n",
    "            xss.append(xs); yss.append(ys)\n",
    "    xss = np.asarray(xss); yss = np.asarray(yss)\n",
    "\n",
    "    sum_bouts = [flies[fly].fictrac.fictrac['Yh'][bout[align_to]-before:bout[align_to]+after] for bout in all_bouts[bout_type]]#[1:-1]\n",
    "    sum_bouts = np.asarray(sum_bouts)\n",
    "    #avg_bout = np.mean(sum_bouts,axis=0)\n",
    "\n",
    "    return xss, yss, sum_bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use np.digitize to bin these timepoints and finally average them (bin the neural data and average it)\n",
    "before = 3000 #in ms\n",
    "after = 3000 # in ms\n",
    "before = int(before/10) # now everything is in units of 10ms\n",
    "after = int(after/10)\n",
    "plt.plot(xss[cluster_num,:,:].ravel(),yss[cluster_num,:,:].ravel(),marker=',',linestyle='',color='k') # for plotting individual neural points\n",
    "\n",
    "neural_bin_size = 100\n",
    "neural_bins = np.arange(-before*10,after*10,neural_bin_size)\n",
    "bin_id = np.digitize(xss[cluster_num,:,:].ravel(), neural_bins)\n",
    "avgs = []\n",
    "for i in range(len(neural_bins)):\n",
    "    avgs.append(np.mean(yss[cluster_num,:,:].ravel()[np.where(bin_id==i)[0]]))\n",
    "plt.plot(neural_bins-0.5*neural_bin_size,avgs,linewidth=1,color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### DEFINE NEEDED FUNCTIONS ###\n",
    "def get_stimulus_metadata(vision_path):\n",
    "\n",
    "    ### try to get from pickle ###\n",
    "    pickle_path = os.path.join(vision_path, 'stimulus_metadata.pkl')\n",
    "    if os.path.exists(pickle_path):\n",
    "        print(\"Loaded from Pickle.\")\n",
    "        with open(pickle_path, 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        return metadata['stim_ids'], metadata['angles']\n",
    "\n",
    "    ### if no pickle, load from .h5 and save pickle for future ###\n",
    "    print(\"No pickle; parsing visprotocol .h5\")\n",
    "    fname = [x for x in os.listdir(vision_path) if '.hdf5' in x][0]\n",
    "    visprotocol_file = os.path.join(vision_path, fname)\n",
    "\n",
    "    stim_ids = []\n",
    "    angles = []\n",
    "    with h5py.File(visprotocol_file, 'r') as f:\n",
    "\n",
    "        ### loop over flies and series to find the one that has many stim presentations (others were aborted)\n",
    "        # note it is critical each fly has their own .h5 file saved\n",
    "        fly_ids = list(f['Flies'].keys())\n",
    "        print(\"Found fly ids: {}\".format(fly_ids))\n",
    "        for fly_id in fly_ids:\n",
    "\n",
    "            series = list(f['Flies'][fly_id]['epoch_runs'].keys())\n",
    "            print(\"Found series: {}\".format(series))\n",
    "            for serie in series:\n",
    "\n",
    "                epoch_ids = f['Flies'][fly_id]['epoch_runs'][serie].get('epochs').keys()\n",
    "                print(str(len(epoch_ids)))\n",
    "                for i, epoch_id in enumerate(epoch_ids):\n",
    "                    stim_id = f['Flies'][fly_id]['epoch_runs'][serie].get('epochs').get(epoch_id).attrs['component_stim_type']\n",
    "                    stim_ids.append(stim_id)\n",
    "                    if stim_id == 'DriftingSquareGrating':\n",
    "                        angle = f['Flies'][fly_id]['epoch_runs'][serie].get('epochs').get(epoch_id).attrs['angle']\n",
    "                        angles.append(angle)\n",
    "                    else:\n",
    "                        angles.append(None)\n",
    "\n",
    "                if len(stim_ids) > 100:\n",
    "\n",
    "                    ### save pickle for next time\n",
    "                    metadata = {'stim_ids': stim_ids, 'angles': angles}\n",
    "                    save_file = os.path.join(vision_path, 'stimulus_metadata.pkl')\n",
    "                    with open(save_file, 'wb') as f:\n",
    "                        pickle.dump(metadata, f)\n",
    "                    print(\"created {}\".format(save_file))\n",
    "\n",
    "                    return stim_ids, angles\n",
    "        print('Could not get visual metadata.')\n",
    "\n",
    "def load_slice(brain_path, slice_num):\n",
    "    with h5py.File(brain_path, 'r') as hf:\n",
    "        single_slice = hf['data'][:,:,slice_num,:]\n",
    "    return single_slice\n",
    "\n",
    "#bin_start and bin_end should be in ms--time before and after stim is presented\n",
    "def data_around_stim(z, bin_start, bin_end, timestamps, list_in_ms, angle):\n",
    "    iri = timestamps[1,z]-timestamps[0,z] #inter recording interval in ms\n",
    "    num_pre_neural_points = int(bin_start/iri)+1\n",
    "    num_post_neural_points = int(bin_end/iri)\n",
    "    single_slice = load_slice(brain_path, z)\n",
    "    data_dic = {'xs': [],'ys': []} #dic that includes neural data and relative time arrays for both stimuli\n",
    "    for i in range(len(list_in_ms[angle])):\n",
    "        # and for each presentation use np.searchsorted to find the nearest real neural datapoint.\n",
    "        nearest = np.searchsorted(timestamps[:,z], list_in_ms[angle][i])\n",
    "        #print(f'stim_time:{list_in_ms[angle][i]} & timestamp:{timestamps[nearest,20]}')\n",
    "        offset = timestamps[nearest, z]-list_in_ms[angle][i]\n",
    "        xs = np.arange(offset-num_pre_neural_points*iri, offset+num_post_neural_points*iri, iri)\n",
    "        #grab this datapoint as well as the flanking neural data\n",
    "        ys = single_slice[:, :, nearest-num_pre_neural_points:nearest+num_post_neural_points]\n",
    "        if np.shape(ys)[-1] == len(xs):\n",
    "            data_dic['xs'].append(xs); data_dic['ys'].append(ys)\n",
    "    xss = np.asarray(data_dic['xs'])\n",
    "    yss = np.asarray(data_dic['ys'])\n",
    "    return xss, yss\n",
    "\n",
    "def bin_avg(bin_size, bin_start, bin_end, xss, yss):\n",
    "    neural_bins = np.arange(-bin_start,bin_end,bin_size)\n",
    "    bin_idxs = np.digitize(xss.ravel(), neural_bins)\n",
    "    yss = np.swapaxes(yss,0,-2)\n",
    "    yss = np.swapaxes(yss,0,1)\n",
    "    dims = np.shape(yss)\n",
    "    yss = np.reshape(yss, (dims[0],dims[1],dims[2]*dims[3]))\n",
    "    avg_by_bin = []\n",
    "    for i in range(len(neural_bins)):\n",
    "        if np.size(np.where(bin_idxs==i)[0]) != 0:\n",
    "            avg_by_bin.append(np.mean(yss[:,:, np.where(bin_idxs==i)[0]],axis=2))\n",
    "        else:\n",
    "            avg_by_bin.append(np.nan)\n",
    "    return avg_by_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading photodiode data... done\n",
      "Loaded from Pickle.\n",
      "Found 269 presented stimuli.\n",
      "starts_angle_0: 88. starts_angle_180: 88\n",
      "Trying to load timestamp data from hdf5 file.\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "func_path = '/oak/stanford/groups/trc/data/Brezovec/2P_Imaging/20190101_walking_dataset/fly_116/func_0/'\n",
    "###########################\n",
    "### PREP VISUAL STIMULI ###\n",
    "###########################\n",
    "\n",
    "vision_path = os.path.join(func_path, 'visual')\n",
    "\n",
    "### Load Photodiode ###\n",
    "t, ft_triggers, pd1, pd2 = brainsss.load_photodiode(vision_path)\n",
    "stimulus_start_times = brainsss.extract_stim_times_from_pd(pd2, t)\n",
    "\n",
    "### Get Metadata ###\n",
    "stim_ids, angles = get_stimulus_metadata(vision_path)\n",
    "print(F\"Found {len(stim_ids)} presented stimuli.\")\n",
    "\n",
    "# *100 puts in units of 10ms, which will match fictrac\n",
    "starts_angle_0 = [int(stimulus_start_times[i]*100) for i in range(len(stimulus_start_times)) if angles[i] == 0]\n",
    "starts_angle_180 = [int(stimulus_start_times[i]*100) for i in range(len(stimulus_start_times)) if angles[i] == 180]\n",
    "print(F\"starts_angle_0: {len(starts_angle_0)}. starts_angle_180: {len(starts_angle_180)}\")\n",
    "list_in_ms = {'0': [i*10 for i in starts_angle_0],\n",
    "                '180': [i*10 for i in starts_angle_180]}\n",
    "\n",
    "brain_path = os.path.join(func_path, 'functional_channel_2_moco_zscore_highpass.h5')\n",
    "timestamps = brainsss.load_timestamps(os.path.join(func_path, 'imaging'), file='functional.xml')\n",
    "\n",
    "### Get Brain dimensions ###\n",
    "with h5py.File(brain_path, 'r') as hf:\n",
    "        data = hf['data']\n",
    "        dims = np.shape(data)\n",
    "        \n",
    "### Loop over all brain slices and stimuli ####\n",
    "avg_by_stim = {'0': [], '180': []}\n",
    "for z in range(dims[-2]):\n",
    "    for angle in list_in_ms.keys():\n",
    "        bin_start = 2500\n",
    "        bin_end = 5500\n",
    "        bin_size = 100\n",
    "        xss, yss = data_around_stim(z, bin_start, bin_end, timestamps, list_in_ms, angle)\n",
    "        avg = bin_avg(bin_size, bin_start, bin_end, xss, yss)\n",
    "        if angle == '0':\n",
    "            avg_by_stim['0'].append(avg)\n",
    "        else:\n",
    "            avg_by_stim['180'].append(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/ilanazs/.local/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49, 80)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: figure out why shape is wrong (should be 88 is 80)\n",
    "np.shape(avg_by_stim['180'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
